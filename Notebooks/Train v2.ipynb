{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training folder data size: 10675\n",
      "Testing folder data size: 1377\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pretrainedmodels.utils as utils\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from apex import amp\n",
    "from easydict import EasyDict\n",
    "from numpy.random import choice, seed\n",
    "from PIL import Image\n",
    "from pretrainedmodels import xception\n",
    "from torch import nn\n",
    "from torch import from_numpy\n",
    "from torch.utils.data import Dataset, SequentialSampler\n",
    "from torchvision import transforms as tt\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm.tqdm = tqdm_notebook\n",
    "tqdm = tqdm_notebook\n",
    "# from unet.unet_model import UNet\n",
    "from segmentation_models_pytorch import Unet\n",
    "from custom_pytorch import custom_layers\n",
    "from custom_pytorch.custom_models import SamplingSegmentationV3 as SamplingSegmentation\n",
    "from custom_pytorch.custom_samplers import SubsetRandomSampler\n",
    "from custom_pytorch.custom_schedulers import CyclicLRWithRestarts\n",
    "from custom_pytorch.custom_losses.dice import *\n",
    "from custom_pytorch.metrics import DiceCoeff\n",
    "from custom_pytorch.custom_utils import params_number, submodules_number\n",
    "from custom_pytorch.custom_visualizations.segmentation import Visualizer\n",
    "from custom_pytorch.custom_utils import get_model_name\n",
    "from custom_pytorch.optimizers.adabound import AdaBound\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "# dice_loss = GeneralizedDiceLoss()\n",
    "import PyQt5\n",
    "sys.path.insert(0, '../')\n",
    "from shared import *\n",
    "from config import CONFIG\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_pytorch.custom_models import SEXceptionXUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "ACTIVATION = 'sigmoid'\n",
    "# create segmentation model with pretrained encoder\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=ENCODER, \n",
    "#     encoder_weights=ENCODER_WEIGHTS, \n",
    "#     classes=1, \n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "# model.name = 'unet'\n",
    "sample = get_dataset('train')[0]['image']\n",
    "sample = sample.unsqueeze(dim=0)\n",
    "\n",
    "from custom_pytorch.models.efficient_net_encoder import EfficientNetEncoder\n",
    "\n",
    "\n",
    "from efficientunet import *\n",
    "\n",
    "encoder = EfficientNetEncoder('efficientnet-b3', pretrained=True).cuda()\n",
    "CONFIG.identifier='EfficientNet_XUnet'\n",
    "\n",
    "# model = SamplingSegmentation(3, 1, 5, 2)\n",
    "\n",
    "# model = SEXceptionXUnet(ENCODER, sample, 1, reversed_features=True).cuda()\n",
    "model = SEXceptionXUnet(encoder, sample, 1, reversed_features=False).cuda()\n",
    "# model = torch.load(\"best_EfficientNet_XUnet_D_2019-08-21_03:41:13.637327_Size_256_Ep_6_TL_6.86_VL_8.57_TM_0.24_VM_0.11.pth\")\n",
    "# model = SamplingSegmentation(3, 1, CONFIG.net_params.depth, CONFIG.net_params.resolution).cuda()\n",
    "# net = NeuralNet().cuda()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Total number of trainable parameters:\", par_num(model))\n",
    "# print(\"Total number of encoder parameters:\", par_num(model.encoder))\n",
    "# print(\"Total number of decoder parameters:\", par_num(model.decoder))\n",
    "# print(\"Total number of output layer parameters:\", par_num(model.output_layer))\n",
    "# for c, col in enumerate(model.decoder['decoding_columns']):\n",
    "#     print('')\n",
    "#     print('Column ', c)\n",
    "#     print(\"Number of decoders: \", len(col.column_decoders))\n",
    "#     print(\"Dimensions of decoders: \", [sub_num(dec) for dec in col.column_decoders])\n",
    "#     print(\"Parameters of decoders: \", [par_num(dec) for dec in col.column_decoders])\n",
    "#     print(\"Number of downsamplers: \", len(col.column_downsamplers))\n",
    "#     print(\"Dimensions of downsamplers: \", [sub_num(dec) for dec in col.column_downsamplers])\n",
    "#     print(\"Number of modules: \", sub_num(col))\n",
    "#     column_parameters = filter(lambda p: p.requires_grad, col.parameters())\n",
    "#     params = sum([np.prod(p.size()) for p in column_parameters])\n",
    "#     print(\"Number of parameters: \", params)\n",
    "# print('')\n",
    "# print(\"Dimensions of output layer: \", sub_num(model.output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding_column_ind = 1\n",
    "# column_decoder_ind = 0\n",
    "# {key: (value.size(), np.prod(value.size())) for key, value in model.decoder['decoding_columns'][decoding_column_ind].column_decoders[column_decoder_ind].named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.decoder['decoding_columns'][decoding_column_ind].column_decoders[column_decoder_ind].reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from custom_pytorch.external.onecyclelr import OneCycleLR\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    lr = CONFIG.lr\n",
    "    momentum = CONFIG.momentum\n",
    "    \n",
    "    # scheduler = AdaptiveCyclicCosineAnnealing(CONFIG.lr,  optimizer, 200, CONFIG.train_batches_num, snapshots_num=8)\n",
    "\n",
    "\n",
    "    train_losses = {}\n",
    "    train_coeffs = {}\n",
    "    valid_losses = {}\n",
    "    valid_coeffs = {}\n",
    "    step = 0\n",
    "    run_validation_every_n_steps = 100\n",
    "\n",
    "    examples_savedir = os.path.join(LOGS_SAVE_DIR, 'visual')\n",
    "    try:\n",
    "        os.makedirs(examples_savedir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    VISUAL_EXAMPLES_EVERY = 50\n",
    "    TRAIN_PLOT_EVERY = 50\n",
    "    partial_loss = 0\n",
    "    partial_coeff = 0\n",
    "\n",
    "    # visualizer = Visualizer(CONFIG, metric_used='Dice Coefficient', include_lr=True, examples_savedir=examples_savedir)\n",
    "\n",
    "#     loss = CEDiceLoss(with_logits=True)\n",
    "#     loss = WindowedCEDiceLoss()\n",
    "#     loss = MultiWindowedBCELoss(with_logits=True, win_num=20)\n",
    "#     loss = BCEAndMultiWindowedDiceLoss(with_logits=True, dice_loss_kwargs={'win_num': 10})\n",
    "#     loss = MultiWindowedDiceLoss(with_logits=True, win_num=10)\n",
    "    loss = BCEAndDiceLoss(with_logits=True)\n",
    "#     loss = ExpandedBCEDiceLoss(with_logits=True, win_num=10)\n",
    "#     loss = MultiWindowedDiceLoss(with_logits=True, win_num=10)\n",
    "    \n",
    "    metrics = [\n",
    "            smp.utils.metrics.IoUMetric(eps=1.),\n",
    "            smp.utils.metrics.FscoreMetric(eps=1.),\n",
    "        ]\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=0.0001)\n",
    "    early_epochs = 0\n",
    "#   scheduler = OneCycleLR(optimizer, num_steps=early_epochs, lr_range=(lr, min(1, 10 * lr)))\n",
    "    trainer = Trainer(model, optimizer, loss, metrics)\n",
    "    trainer.load_model('EfficientNet_XUnet_D_2019-08-21_23:19:17.405013_Size_256_Ep_1_TL_1.99_VL_2.13_TM_0.02_VM_0.01best.pth')\n",
    "\n",
    "    # train model for 40 epochs\n",
    "\n",
    "    max_score = 0\n",
    "    stale_epochs_num = 0\n",
    "    min_loss = 10000\n",
    "    ALLOWED_STALE = 2 * CONFIG.restart_period\n",
    "    SWA_START = 4 * CONFIG.restart_period\n",
    "    best_model_name = None\n",
    "    while True:\n",
    "        if trainer.epoch == early_epochs:\n",
    "#             print(\"\\n\\nOne cycle policy finished, starting Cyclic LR schedule with lower LR\\n\\n\")\n",
    "#             trainer.optimizer = SWA(optimizer, \n",
    "#                                     swa_start=SWA_START * CONFIG.train_batches_number,\n",
    "#                                     swa_freq=2 * CONFIG.train_batches_number,\n",
    "#                                     swa_lr=0.1 * CONFIG.lr)\n",
    "#             trainer.optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG.lr)\n",
    "#             scheduler = CyclicLRWithRestarts(trainer.optimizer, CONFIG.batch_size, CONFIG.train_size, restart_period=CONFIG.restart_period,\n",
    "#                                           t_mult=1.2, min_lr= 1e-2 * CONFIG.lr)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(trainer.optimizer, mode='min', verbose=True)\n",
    "\n",
    "        print('\\nEpoch: {}, LR: {}'.format(trainer.epoch + 1, trainer.optimizer.param_groups[0]['lr']))\n",
    "        train_logs = trainer.step(logs=trainer.train_loss_logs, valid=False)\n",
    "        trainer.write_logs(train_logs, valid=False)\n",
    "        valid_logs = trainer.step(logs=trainer.valid_loss_logs, valid=True)\n",
    "        trainer.write_logs(valid_logs, valid=True)\n",
    "        \n",
    "\n",
    "\n",
    "        # do something (save model, change lr, etc.)\n",
    "        stale_epoch = True\n",
    "        if max_score < valid_logs['iou']:\n",
    "            stale_epochs_num = 0\n",
    "            max_score = valid_logs['iou']\n",
    "            trainer.save_best_model(valid_metric=max_score, train_metric=train_logs['iou'], valid_loss=valid_logs[loss.__name__], train_loss=train_logs[loss.__name__])\n",
    "            print('Model saved!')\n",
    "            stale_epoch = False\n",
    "        if valid_logs[loss.__name__] < min_loss:\n",
    "            min_loss = valid_logs[loss.__name__]\n",
    "            stale_epoch = False\n",
    "            print(\"Minimum valid loss was found, the stale counter has reset.\")\n",
    "        if stale_epoch:\n",
    "            stale_epochs_num += 1\n",
    "        if stale_epochs_num == ALLOWED_STALE:\n",
    "            print(f\"No improvements for the last {ALLOWED_STALE} epochs, the training has finished\")\n",
    "            print(f\"Best validation IoU score: {max_score}\")\n",
    "            break\n",
    "        scheduler.step(train_logs[loss.__name__])\n",
    "#         if i < SWA_START:\n",
    "#             scheduler.step()\n",
    "#     trainer.optimizer.swap_swa_sgd()\n",
    "    trainer.save_last_model(valid_metric=max_score, train_metric=train_logs['iou'], valid_loss=valid_logs[loss.__name__], train_loss=train_logs[loss.__name__])\n",
    "    return best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12858504"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_number(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submodules_number(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high weight indices found:  2102\n",
      "The following attributes of the configuration differ from the ones loaded:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loaded</th>\n",
       "      <th>current</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2019-08-21 23:19:17.405013</td>\n",
       "      <td>2019-08-21 23:26:51.697879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_batches_number</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_size</th>\n",
       "      <td>20</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          loaded                     current\n",
       "date                  2019-08-21 23:19:17.405013  2019-08-21 23:26:51.697879\n",
       "train_batches_number                           5                         500\n",
       "train_size                                    20                        2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, LR: 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f54e780236a40a9a9d05add1a85df18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-24e5fb1ba746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-ad40420be95c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: {}, LR: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mvalid_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loss_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/kaggle/custom-pytorch/custom_pytorch/custom_utils/train.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(logs, valid)\u001b[0m\n\u001b[1;32m     54\u001b[0m             self.valid_loader, inp_index, gt_index, _logs=logs)\n\u001b[1;32m     55\u001b[0m         self.step = lambda logs, valid: self.train_step(\n\u001b[0;32m---> 56\u001b[0;31m             logs) if not valid else self.valid_step(logs)\n\u001b[0m\u001b[1;32m     57\u001b[0m         self.snasphots_handler = SnapshotsHandler(\n\u001b[1;32m     58\u001b[0m             self, 'snapshots', create_dir=True)\n",
      "\u001b[0;32m~/Code/kaggle/custom-pytorch/custom_pytorch/custom_utils/train.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(logs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             model, loss_function, metric_functions, device, verbose)\n\u001b[1;32m     51\u001b[0m         self.train_step = lambda logs: self.train_epoch.run(\n\u001b[0;32m---> 52\u001b[0;31m             self.train_loader, inp_index, gt_index, _logs=logs)\n\u001b[0m\u001b[1;32m     53\u001b[0m         self.valid_step = lambda logs: self.valid_epoch.run(\n\u001b[1;32m     54\u001b[0m             self.valid_loader, inp_index, gt_index, _logs=logs)\n",
      "\u001b[0;32m~/Code/kaggle/custom-pytorch/custom_pytorch/custom_utils/epoch.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataloader, inp_index, gt_index, _logs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgt_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/kaggle/custom-pytorch/custom_pytorch/custom_utils/epoch.py\u001b[0m in \u001b[0;36mbatch_update\u001b[0;34m(self, x, y, logs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/kaggle/.venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/kaggle/.venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model_name = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training model using same LR for both encoder and decoder\")\n",
    "# model = torch.load(\"best_xUnet_encoder_D_2019-07-27_01:35:05.984560_Ep_129_TL_0.49_VL_0.48_TM_0.57_VM_0.57.pth\")\n",
    "model = torch.load(best_model_name)\n",
    "best_model_name = train_model(mainly_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model)\n",
    "best_model_name = train_model(mainly_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(best_model_name)\n",
    "best_model_name = train_model(mainly_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
